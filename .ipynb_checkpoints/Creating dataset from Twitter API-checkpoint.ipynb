{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis of tweets from top 20+ Silicon Valley (SV) influencers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data science toolbox\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Access to API\n",
    "import tweepy\n",
    "\n",
    "# Other\n",
    "import json\n",
    "\n",
    "# Wildcard operations on files\n",
    "import glob\n",
    "\n",
    "# Date & time operations\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://www.digitaltrends.com/social-media/tech-people-influencers-follow-twitter/\n",
    "# Second source: https://www.quora.com/What-are-some-of-the-best-Silicon-Valley-Twitter-accounts-to-follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There are together 26 Twitter accounts from which we'll build dataset\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persons = set(['tim_cook', 'sundarpichai', 'billgates', \n",
    "           'elonmusk', 'jeffbezos', 'emilychangtv',\n",
    "          'mkbhd', 'dhh', 'reshmasaujani',\n",
    "          'KaraSwisher', 'mims', 'davidcohen',\n",
    "          'charlesarthur', 'jeffweiner', 'benedictevans',\n",
    "          'sirajraval', 'ajitpaifcc', 'BoredElonMusk', \n",
    "          'SwiftonSecurity', 'DigitalTrends',\n",
    "          'cdixon', 'JonErlichman', 'fxshaw', 'ericjackson', 'asymco', 'MikeIsaac'])\n",
    "\n",
    "f\"There are together {len(persons)} Twitter accounts from which we'll build dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We'll analyze tweets in the time period between 2018-08-10 00:00:00 end 2018-09-08 23:59:59 (including)\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date = datetime.datetime(year=2018, month=8, day=10)\n",
    "end_date = datetime.datetime(year=2018, month=9, day=8, hour=23, minute=59, second=59)\n",
    "\n",
    "f\"We'll analyze tweets in the time period between {start_date} end {end_date} (including)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare credentials for querying API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter API credentials\n",
    "# # (I have used here my personal consumer_key, consumer_secret, access_key, access_secret)\n",
    "# because of the limits in tweeter API i had to created more pair of my credentials. I will use them to get\n",
    "# the acces to all tweets I need\n",
    "\n",
    "rotating_credentials = [\n",
    "(\"fDmPUsiDAxZkIOVBjpck4o1n3\", \"EcNK9UfMe4fiaXFwvMWDjfReSviksgxFy3UhQ9yvYvFveFQtV7\", \n",
    " \"914842765-StdZ8LjQGQm6gQgVbxMitC5cDTNDwjMs4fH2jfPR\", \"p9peNw0BKy1GI8vOtBfW6gVqNhSuNYvokipHjYLC84hPc\"),\n",
    "(\"tnnfyogABdQgYQAMjeAwhMSRu\", \"awCxjFKBpXA8GApZJcyqYCIwQnXVsSWSuJLQKbCZ9YvhC33uC4\", \n",
    "\"914842765-kur9whOqACbW7pJC1rrybJeUgAbme3rYfmOEcZgO\", \"gIAbk5D4DVqsbwFwzLxneOXGyQHVjnWtJGTQlg1361TLO\"),\n",
    "(\"Mryk9xUD0voaG625GLMVfHzBw\", \"c34WK2T7SmNNiT2CgQG4StWl47Yf9dSxxrPUG6izu63SgihQDs\",\n",
    "\"914842765-XXwDxcP4vZcCAf5k7gDA5kuO77aBOpUp39rbJM7X\", \"FBHcy1x7MUPMQ8VClnTKYs8hzVdJ8RG9zfsaUWCz3yOZj\"),\n",
    "(\"6irsVtCQZQGMexYh2swanAUFS\", \"kuPho0piwZQLo6HulcZnJyUSbQpw63zeVIomysQaie85GwW01O\",\n",
    "\"914842765-VOFoZWHDxX86WULBLUlmOuYeKSjbnPhs2KuQoBNa\", \"lJj2FN2Q9mXYYIGQuIRBJRLN0MgbfUiQ9DvFH7AiMfoZF\")]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_instances = []\n",
    "\n",
    "for credential in rotating_credentials:\n",
    "    auth = tweepy.OAuthHandler(credential[0], credential[1])\n",
    "    auth.set_access_token(credential[2], credential[3])\n",
    "    api = tweepy.API(auth)\n",
    "    api_instances.append(api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read follower count statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "follower_stat_cols = ['nick', 'date', 'delta_followers', 'followers_number']\n",
    "followers_stats = pd.DataFrame(columns=follower_stat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./dataset/followers_stats/*.xlsx\"  # asterix notation\n",
    "\n",
    "'''\n",
    "Followers stat were created using 3rd party webpage holding followers statistics from a last month:\n",
    "https://socialblade.com/twitter/user/elonmusk/monthly\n",
    "We made single excel file per user holding change in number of followers\n",
    "(see the /dataset/followers_stats directory for raw data)\n",
    "'''\n",
    "for filename in glob.glob(path):\n",
    "    nick = filename.split('\\\\')[1].split('.xlsx')[0]\n",
    "    \n",
    "    followers = pd.read_excel(filename, header=None)\n",
    "    followers_raw = []\n",
    "    current_date = None\n",
    "    delta_followers = None\n",
    "    followers_number = None\n",
    "    \n",
    "    for index, row in followers.iterrows():\n",
    "        # print(row[0])\n",
    "        if index % 8 == 0:\n",
    "            current_date = row[0]\n",
    "        elif index % 8 == 2:\n",
    "            delta_followers = row[0]\n",
    "        elif index % 8 == 3:\n",
    "            followers_number = row[0]\n",
    "        elif index % 8 == 7:\n",
    "            followers_raw.append((nick, current_date, delta_followers, followers_number))\n",
    "    \n",
    "    # print(f'{nick} parsed, {len(followers_raw)} rows read')\n",
    "    \n",
    "    followers_stats = pd.concat([followers_stats, pd.DataFrame.from_records(followers_raw, columns=follower_stat_cols)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# followers_stats.groupby(['nick']).date.agg(['count'])\n",
    "len(followers_stats.groupby(['nick']).date.agg(['count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nick                object\n",
       "date                object\n",
       "delta_followers     object\n",
       "followers_number    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "followers_stats.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter API parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### getting content of the tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tweets(screen_name):\n",
    "    \n",
    "    iterator = 0\n",
    "    \n",
    "    api = api_instances[iterator % 4]\n",
    "        \n",
    "    alltweets = []  # here are stored all results of parse\n",
    "    \n",
    "    new_tweets = api.user_timeline(screen_name=screen_name, \n",
    "                                   count=200, \n",
    "                                   tweet_mode='extended')  # allows to get more than 140 chars\n",
    "    \n",
    "    alltweets.extend(new_tweets)  # adding new tweets to all tweets\n",
    "\n",
    "    oldest = alltweets[-1].id - 1\n",
    "\n",
    "    while len(new_tweets) > 0:\n",
    "        \n",
    "        iterator += 1\n",
    "        \n",
    "        if alltweets[-1].created_at < start_date:\n",
    "            print('Time threshold of {} reached, going to next Twitter account'.format(start_date))\n",
    "            break\n",
    "        \n",
    "        api = api_instances[iterator % 4]\n",
    "        \n",
    "        print('Credentials rotated due iterator {}, key is {}'.format(iterator, rotating_credentials[iterator % 4][0]))\n",
    "\n",
    "        new_tweets = api.user_timeline(screen_name=screen_name, count=200, tweet_mode='extended', max_id=oldest)\n",
    "\n",
    "        alltweets.extend(new_tweets)\n",
    "\n",
    "        oldest = alltweets[-1].id - 1\n",
    "        oldest_object = alltweets[-1]\n",
    "\n",
    "        print('Downloaded so far {} tweets for user {}'.format(len(alltweets), screen_name))\n",
    "\n",
    "    # Here I already finished getting tweet data from API\n",
    "    # Below is creating list of tuples holding data from columns (part of tweet object)\n",
    "    \n",
    "    formated_tweets = [(screen_name, tweet.id_str, tweet.created_at,  \n",
    "                        tweet.full_text.encode('utf-8'), \n",
    "                        tweet.geo, tweet.place, tweet.is_quote_status, \n",
    "                        len(tweet.entities['hashtags']) > 0, len(tweet.entities['symbols']) > 0, \n",
    "                        len(tweet.entities['user_mentions']) > 0, len(tweet.entities['urls']) > 0,\n",
    "                        len(tweet.entities['hashtags']), len(tweet.entities['symbols']), \n",
    "                        len(tweet.entities['user_mentions']), len(tweet.entities['urls']),\n",
    "                        tweet.coordinates, tweet.contributors, \n",
    "                        tweet.retweet_count, tweet.favorite_count,\n",
    "                        tweet.favorited, tweet.retweeted, tweet.lang,\n",
    "                        tweet.in_reply_to_status_id, tweet.in_reply_to_status_id_str,\n",
    "                        tweet.in_reply_to_user_id, tweet.in_reply_to_user_id_str,\n",
    "                        tweet.in_reply_to_screen_name) for tweet in alltweets]\n",
    "    \n",
    "    # there must be defined columns for the future pandas data frame\n",
    "    \n",
    "    labels = ['screen_name', 'id', 'created_at', \n",
    "              'text', \n",
    "              'geo', 'place', 'is_quote_status', \n",
    "              'has_hashtags', 'has_symbols', 'has_user_mentions', 'has_urls', \n",
    "              'hashtags_count', 'symbols_count', 'user_mentions_count', 'urls_count',\n",
    "              'coordinates', 'contributors',  \n",
    "              'retweet_count', 'favorite_count', \n",
    "              'favorited', 'retweeted', 'lang', \n",
    "              'in_reply_to_status_id', 'in_reply_to_status_id_str', \n",
    "              'in_reply_to_user_id', 'in_reply_to_user_id_str', 'in_reply_to_screen_name']\n",
    "    \n",
    "    result = pd.DataFrame.from_records(formated_tweets, columns=labels)  # accepts list of tuples (here, formated_tweets)\n",
    "    \n",
    "    return result  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lunch downloading tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing twitter handle davidcohen\n",
      "Time threshold of 2018-08-10 00:00:00 reached, going to next Twitter account\n",
      "get_all_tweets done job! length:200\n",
      "Processing twitter handle charlesarthur\n",
      "Credentials rotated due iterator 1, key is tnnfyogABdQgYQAMjeAwhMSRu\n",
      "Downloaded so far 400 tweets for user charlesarthur\n",
      "Credentials rotated due iterator 2, key is Mryk9xUD0voaG625GLMVfHzBw\n",
      "Downloaded so far 600 tweets for user charlesarthur\n",
      "Credentials rotated due iterator 3, key is 6irsVtCQZQGMexYh2swanAUFS\n",
      "Downloaded so far 800 tweets for user charlesarthur\n",
      "Credentials rotated due iterator 4, key is fDmPUsiDAxZkIOVBjpck4o1n3\n",
      "Downloaded so far 1000 tweets for user charlesarthur\n",
      "Credentials rotated due iterator 5, key is tnnfyogABdQgYQAMjeAwhMSRu\n",
      "Downloaded so far 1199 tweets for user charlesarthur\n",
      "Credentials rotated due iterator 6, key is Mryk9xUD0voaG625GLMVfHzBw\n",
      "Downloaded so far 1399 tweets for user charlesarthur\n",
      "Credentials rotated due iterator 7, key is 6irsVtCQZQGMexYh2swanAUFS\n",
      "Downloaded so far 1599 tweets for user charlesarthur\n",
      "Credentials rotated due iterator 8, key is fDmPUsiDAxZkIOVBjpck4o1n3\n",
      "Downloaded so far 1799 tweets for user charlesarthur\n",
      "Time threshold of 2018-08-10 00:00:00 reached, going to next Twitter account\n",
      "get_all_tweets done job! length:1799\n",
      "Processing twitter handle ericjackson\n",
      "Credentials rotated due iterator 1, key is tnnfyogABdQgYQAMjeAwhMSRu\n",
      "Downloaded so far 400 tweets for user ericjackson\n",
      "Credentials rotated due iterator 2, key is Mryk9xUD0voaG625GLMVfHzBw\n",
      "Downloaded so far 600 tweets for user ericjackson\n",
      "Credentials rotated due iterator 3, key is 6irsVtCQZQGMexYh2swanAUFS\n",
      "Downloaded so far 800 tweets for user ericjackson\n",
      "Credentials rotated due iterator 4, key is fDmPUsiDAxZkIOVBjpck4o1n3\n",
      "Downloaded so far 999 tweets for user ericjackson\n",
      "Time threshold of 2018-08-10 00:00:00 reached, going to next Twitter account\n",
      "get_all_tweets done job! length:999\n",
      "Processing twitter handle emilychangtv\n",
      "Time threshold of 2018-08-10 00:00:00 reached, going to next Twitter account\n",
      "get_all_tweets done job! length:200\n",
      "Processing twitter handle DigitalTrends\n",
      "Credentials rotated due iterator 1, key is tnnfyogABdQgYQAMjeAwhMSRu\n",
      "Downloaded so far 400 tweets for user DigitalTrends\n",
      "Credentials rotated due iterator 2, key is Mryk9xUD0voaG625GLMVfHzBw\n",
      "Downloaded so far 600 tweets for user DigitalTrends\n",
      "Credentials rotated due iterator 3, key is 6irsVtCQZQGMexYh2swanAUFS\n",
      "Downloaded so far 800 tweets for user DigitalTrends\n",
      "Credentials rotated due iterator 4, key is fDmPUsiDAxZkIOVBjpck4o1n3\n",
      "Downloaded so far 1000 tweets for user DigitalTrends\n",
      "Credentials rotated due iterator 5, key is tnnfyogABdQgYQAMjeAwhMSRu\n",
      "Downloaded so far 1200 tweets for user DigitalTrends\n",
      "Credentials rotated due iterator 6, key is Mryk9xUD0voaG625GLMVfHzBw\n",
      "Downloaded so far 1400 tweets for user DigitalTrends\n",
      "Credentials rotated due iterator 7, key is 6irsVtCQZQGMexYh2swanAUFS\n",
      "Downloaded so far 1600 tweets for user DigitalTrends\n",
      "Credentials rotated due iterator 8, key is fDmPUsiDAxZkIOVBjpck4o1n3\n",
      "Downloaded so far 1800 tweets for user DigitalTrends\n",
      "Credentials rotated due iterator 9, key is tnnfyogABdQgYQAMjeAwhMSRu\n",
      "Downloaded so far 2000 tweets for user DigitalTrends\n",
      "Credentials rotated due iterator 10, key is Mryk9xUD0voaG625GLMVfHzBw\n",
      "Downloaded so far 2200 tweets for user DigitalTrends\n",
      "Credentials rotated due iterator 11, key is 6irsVtCQZQGMexYh2swanAUFS\n",
      "Downloaded so far 2400 tweets for user DigitalTrends\n",
      "Time threshold of 2018-08-10 00:00:00 reached, going to next Twitter account\n",
      "get_all_tweets done job! length:2400\n",
      "Processing twitter handle benedictevans\n",
      "Credentials rotated due iterator 1, key is tnnfyogABdQgYQAMjeAwhMSRu\n",
      "Downloaded so far 400 tweets for user benedictevans\n",
      "Credentials rotated due iterator 2, key is Mryk9xUD0voaG625GLMVfHzBw\n",
      "Downloaded so far 599 tweets for user benedictevans\n",
      "Credentials rotated due iterator 3, key is 6irsVtCQZQGMexYh2swanAUFS\n",
      "Downloaded so far 799 tweets for user benedictevans\n",
      "Credentials rotated due iterator 4, key is fDmPUsiDAxZkIOVBjpck4o1n3\n",
      "Downloaded so far 998 tweets for user benedictevans\n",
      "Credentials rotated due iterator 5, key is tnnfyogABdQgYQAMjeAwhMSRu\n",
      "Downloaded so far 1197 tweets for user benedictevans\n",
      "Credentials rotated due iterator 6, key is Mryk9xUD0voaG625GLMVfHzBw\n",
      "Downloaded so far 1396 tweets for user benedictevans\n",
      "Credentials rotated due iterator 7, key is 6irsVtCQZQGMexYh2swanAUFS\n",
      "Downloaded so far 1595 tweets for user benedictevans\n",
      "Time threshold of 2018-08-10 00:00:00 reached, going to next Twitter account\n",
      "get_all_tweets done job! length:1595\n",
      "Processing twitter handle sirajraval\n",
      "Credentials rotated due iterator 1, key is tnnfyogABdQgYQAMjeAwhMSRu\n",
      "Downloaded so far 400 tweets for user sirajraval\n",
      "Time threshold of 2018-08-10 00:00:00 reached, going to next Twitter account\n",
      "get_all_tweets done job! length:400\n",
      "Processing twitter handle KaraSwisher\n",
      "Credentials rotated due iterator 1, key is tnnfyogABdQgYQAMjeAwhMSRu\n",
      "Downloaded so far 400 tweets for user KaraSwisher\n",
      "Credentials rotated due iterator 2, key is Mryk9xUD0voaG625GLMVfHzBw\n",
      "Downloaded so far 600 tweets for user KaraSwisher\n",
      "Credentials rotated due iterator 3, key is 6irsVtCQZQGMexYh2swanAUFS\n",
      "Downloaded so far 800 tweets for user KaraSwisher\n",
      "Credentials rotated due iterator 4, key is fDmPUsiDAxZkIOVBjpck4o1n3\n",
      "Downloaded so far 1000 tweets for user KaraSwisher\n",
      "Credentials rotated due iterator 5, key is tnnfyogABdQgYQAMjeAwhMSRu\n",
      "Downloaded so far 1200 tweets for user KaraSwisher\n",
      "Credentials rotated due iterator 6, key is Mryk9xUD0voaG625GLMVfHzBw\n",
      "Downloaded so far 1400 tweets for user KaraSwisher\n",
      "Credentials rotated due iterator 7, key is 6irsVtCQZQGMexYh2swanAUFS\n",
      "Downloaded so far 1600 tweets for user KaraSwisher\n",
      "Credentials rotated due iterator 8, key is fDmPUsiDAxZkIOVBjpck4o1n3\n",
      "Downloaded so far 1800 tweets for user KaraSwisher\n",
      "Credentials rotated due iterator 9, key is tnnfyogABdQgYQAMjeAwhMSRu\n",
      "Downloaded so far 2000 tweets for user KaraSwisher\n",
      "Credentials rotated due iterator 10, key is Mryk9xUD0voaG625GLMVfHzBw\n",
      "Downloaded so far 2199 tweets for user KaraSwisher\n",
      "Time threshold of 2018-08-10 00:00:00 reached, going to next Twitter account\n",
      "get_all_tweets done job! length:2199\n",
      "Processing twitter handle jeffbezos\n",
      "Time threshold of 2018-08-10 00:00:00 reached, going to next Twitter account\n",
      "get_all_tweets done job! length:172\n",
      "Processing twitter handle mims\n",
      "Credentials rotated due iterator 1, key is tnnfyogABdQgYQAMjeAwhMSRu\n",
      "Downloaded so far 400 tweets for user mims\n",
      "Credentials rotated due iterator 2, key is Mryk9xUD0voaG625GLMVfHzBw\n",
      "Downloaded so far 600 tweets for user mims\n",
      "Credentials rotated due iterator 3, key is 6irsVtCQZQGMexYh2swanAUFS\n",
      "Downloaded so far 800 tweets for user mims\n",
      "Credentials rotated due iterator 4, key is fDmPUsiDAxZkIOVBjpck4o1n3\n",
      "Downloaded so far 1000 tweets for user mims\n",
      "Credentials rotated due iterator 5, key is tnnfyogABdQgYQAMjeAwhMSRu\n",
      "Downloaded so far 1200 tweets for user mims\n",
      "Time threshold of 2018-08-10 00:00:00 reached, going to next Twitter account\n",
      "get_all_tweets done job! length:1200\n",
      "Processing twitter handle MikeIsaac\n",
      "Credentials rotated due iterator 1, key is tnnfyogABdQgYQAMjeAwhMSRu\n",
      "Downloaded so far 400 tweets for user MikeIsaac\n",
      "Credentials rotated due iterator 2, key is Mryk9xUD0voaG625GLMVfHzBw\n",
      "Downloaded so far 600 tweets for user MikeIsaac\n",
      "Time threshold of 2018-08-10 00:00:00 reached, going to next Twitter account\n",
      "get_all_tweets done job! length:600\n",
      "Processing twitter handle dhh\n",
      "Credentials rotated due iterator 1, key is tnnfyogABdQgYQAMjeAwhMSRu\n",
      "Downloaded so far 400 tweets for user dhh\n",
      "Credentials rotated due iterator 2, key is Mryk9xUD0voaG625GLMVfHzBw\n",
      "Downloaded so far 600 tweets for user dhh\n",
      "Time threshold of 2018-08-10 00:00:00 reached, going to next Twitter account\n",
      "get_all_tweets done job! length:600\n",
      "Processing twitter handle JonErlichman\n",
      "Time threshold of 2018-08-10 00:00:00 reached, going to next Twitter account\n",
      "get_all_tweets done job! length:200\n",
      "Processing twitter handle billgates\n",
      "Time threshold of 2018-08-10 00:00:00 reached, going to next Twitter account\n",
      "get_all_tweets done job! length:200\n",
      "Processing twitter handle ajitpaifcc\n",
      "Credentials rotated due iterator 1, key is tnnfyogABdQgYQAMjeAwhMSRu\n",
      "Downloaded so far 399 tweets for user ajitpaifcc\n",
      "Credentials rotated due iterator 2, key is Mryk9xUD0voaG625GLMVfHzBw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded so far 599 tweets for user ajitpaifcc\n",
      "Credentials rotated due iterator 3, key is 6irsVtCQZQGMexYh2swanAUFS\n",
      "Downloaded so far 799 tweets for user ajitpaifcc\n",
      "Time threshold of 2018-08-10 00:00:00 reached, going to next Twitter account\n",
      "get_all_tweets done job! length:799\n",
      "Processing twitter handle jeffweiner\n",
      "Time threshold of 2018-08-10 00:00:00 reached, going to next Twitter account\n",
      "get_all_tweets done job! length:200\n",
      "Processing twitter handle mkbhd\n",
      "Credentials rotated due iterator 1, key is tnnfyogABdQgYQAMjeAwhMSRu\n",
      "Downloaded so far 399 tweets for user mkbhd\n",
      "Time threshold of 2018-08-10 00:00:00 reached, going to next Twitter account\n",
      "get_all_tweets done job! length:399\n",
      "Processing twitter handle BoredElonMusk\n",
      "Time threshold of 2018-08-10 00:00:00 reached, going to next Twitter account\n",
      "get_all_tweets done job! length:200\n",
      "Processing twitter handle tim_cook\n",
      "Time threshold of 2018-08-10 00:00:00 reached, going to next Twitter account\n",
      "get_all_tweets done job! length:200\n",
      "Processing twitter handle SwiftonSecurity\n",
      "Credentials rotated due iterator 1, key is tnnfyogABdQgYQAMjeAwhMSRu\n",
      "Downloaded so far 400 tweets for user SwiftonSecurity\n",
      "Credentials rotated due iterator 2, key is Mryk9xUD0voaG625GLMVfHzBw\n",
      "Downloaded so far 600 tweets for user SwiftonSecurity\n",
      "Credentials rotated due iterator 3, key is 6irsVtCQZQGMexYh2swanAUFS\n",
      "Downloaded so far 800 tweets for user SwiftonSecurity\n",
      "Credentials rotated due iterator 4, key is fDmPUsiDAxZkIOVBjpck4o1n3\n",
      "Downloaded so far 1000 tweets for user SwiftonSecurity\n",
      "Credentials rotated due iterator 5, key is tnnfyogABdQgYQAMjeAwhMSRu\n",
      "Downloaded so far 1200 tweets for user SwiftonSecurity\n",
      "Credentials rotated due iterator 6, key is Mryk9xUD0voaG625GLMVfHzBw\n",
      "Downloaded so far 1400 tweets for user SwiftonSecurity\n",
      "Time threshold of 2018-08-10 00:00:00 reached, going to next Twitter account\n",
      "get_all_tweets done job! length:1400\n",
      "Processing twitter handle sundarpichai\n",
      "Time threshold of 2018-08-10 00:00:00 reached, going to next Twitter account\n",
      "get_all_tweets done job! length:200\n",
      "Processing twitter handle fxshaw\n",
      "Time threshold of 2018-08-10 00:00:00 reached, going to next Twitter account\n",
      "get_all_tweets done job! length:200\n",
      "Processing twitter handle asymco\n",
      "Credentials rotated due iterator 1, key is tnnfyogABdQgYQAMjeAwhMSRu\n",
      "Downloaded so far 400 tweets for user asymco\n",
      "Credentials rotated due iterator 2, key is Mryk9xUD0voaG625GLMVfHzBw\n",
      "Downloaded so far 600 tweets for user asymco\n",
      "Time threshold of 2018-08-10 00:00:00 reached, going to next Twitter account\n",
      "get_all_tweets done job! length:600\n",
      "Processing twitter handle cdixon\n",
      "Time threshold of 2018-08-10 00:00:00 reached, going to next Twitter account\n",
      "get_all_tweets done job! length:200\n",
      "Processing twitter handle reshmasaujani\n",
      "Time threshold of 2018-08-10 00:00:00 reached, going to next Twitter account\n",
      "get_all_tweets done job! length:200\n",
      "Processing twitter handle elonmusk\n",
      "Time threshold of 2018-08-10 00:00:00 reached, going to next Twitter account\n",
      "get_all_tweets done job! length:200\n"
     ]
    }
   ],
   "source": [
    "result_dict = {}\n",
    "\n",
    "for screen_name in persons:\n",
    "    print('Processing twitter handle {}'.format(screen_name))\n",
    "    \n",
    "    p_tweets = get_all_tweets(screen_name)\n",
    "    print(f'get_all_tweets done job! length:{len(p_tweets)}')\n",
    "    \n",
    "    result_dict[screen_name] = p_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're joining 26 result Pandas dataframe into one\n",
    "# so we can write to single CSV file\n",
    "\n",
    "tweets = [p for p in result_dict.values()]\n",
    "dataset = pd.concat(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge information on followers to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta_f(screen_name, created_at): \n",
    "    global followers_stats\n",
    "    \n",
    "    match = followers_stats[(followers_stats.date.dt.month == created_at.month) & \n",
    "                    (followers_stats.date.dt.day == created_at.day) & \n",
    "                    (followers_stats.nick == screen_name)]\n",
    "    \n",
    "    if len(match) > 0:\n",
    "        delta_found = match.iloc[0].delta_followers\n",
    "        if delta_found == '--':\n",
    "            delta_found = 0\n",
    "        return (delta_found, match.iloc[0].followers_number)\n",
    "    else:\n",
    "        return (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "('Can only use .dt accessor with datetimelike values', 'occurred at index 0')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-3d02b19c0f04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'delta_followers'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mget_delta_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'screen_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'created_at'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, broadcast, raw, reduce, result_type, args, **kwds)\u001b[0m\n\u001b[0;32m   6002\u001b[0m                          \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6003\u001b[0m                          kwds=kwds)\n\u001b[1;32m-> 6004\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[1;31m# compute the result using the series generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;31m# wrap results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m                     \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m                     \u001b[0mkeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-3d02b19c0f04>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'delta_followers'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mget_delta_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'screen_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'created_at'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-3c681b242654>\u001b[0m in \u001b[0;36mget_delta_f\u001b[1;34m(screen_name, created_at)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     match = followers_stats[(followers_stats.date.dt.month == created_at.month) & \n\u001b[1;32m----> 5\u001b[1;33m                     \u001b[1;33m(\u001b[0m\u001b[0mfollowers_stats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mday\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mcreated_at\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mday\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m                     (followers_stats.nick == screen_name)]\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   4366\u001b[0m         if (name in self._internal_names_set or name in self._metadata or\n\u001b[0;32m   4367\u001b[0m                 name in self._accessors):\n\u001b[1;32m-> 4368\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4369\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4370\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\accessor.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[1;31m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         \u001b[0maccessor_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;31m# Replace the property with the accessor object. Inspired by:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[1;31m# http://www.pydanny.com/cached-property.html\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\accessors.py\u001b[0m in \u001b[0;36m__new__\u001b[1;34m(cls, data)\u001b[0m\n\u001b[0;32m    323\u001b[0m             \u001b[1;32mpass\u001b[0m  \u001b[1;31m# we raise an attribute error anyway\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         raise AttributeError(\"Can only use .dt accessor with datetimelike \"\n\u001b[0m\u001b[0;32m    326\u001b[0m                              \"values\")\n",
      "\u001b[1;31mAttributeError\u001b[0m: ('Can only use .dt accessor with datetimelike values', 'occurred at index 0')"
     ]
    }
   ],
   "source": [
    "dataset['delta_followers'] = dataset.apply(lambda x: get_delta_f(x['screen_name'], x['created_at'])[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['followers_count'] = dataset.apply(lambda x: get_delta_f(x['screen_name'], x['created_at'])[1], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('dataset/dataset-sv-201808.csv', index=False)\n",
    "\n",
    "print('Done exporting dataset to CSV files!')\n",
    "print(f'Together {len(dataset)} rows saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conslusions:\n",
    "\n",
    "Dataset exported to single CSV file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
